{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import random\n",
    "\n",
    "import AttnEncoder\n",
    "# Global defs\n",
    "\n",
    "# iters_per_epoch should also be shifted here ?\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 1\n",
    "batch_size = 32\n",
    "\n",
    "MASTER_MAX_LEN = 300\n",
    "MASTER_MAX_VAL_LEN = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function defs\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "def asMinutes(s):\n",
    "\tm = math.floor(s / 60)\n",
    "\ts -= m * 60\n",
    "\treturn '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "\tnow = time.time()\n",
    "\ts = now - since\n",
    "\tes = s / (percent + 1e-8)\n",
    "\trs = es - s\n",
    "\treturn '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ** Starting of the Main code \n",
    "# ** ** \n",
    "# ** ** \n",
    "import re\n",
    "\n",
    "# Convert string to vector of floats\n",
    "def convert_to_float(string): # string with float values separated by spaces\n",
    "\tlis = string.split()\n",
    "\tlis_rating = [ float(value) for value in lis]\n",
    "\treturn lis_rating\n",
    "\n",
    "# Unique index for words\n",
    "index = 0\n",
    "def get_index():\n",
    "\tglobal index\n",
    "\tto_ret = index\n",
    "\tindex += 1\n",
    "\treturn to_ret\n",
    "\n",
    "# Dictionaries\n",
    "dict_ind2vec = {}\n",
    "dict_ind2str = {}\n",
    "dict_str2ind = {}\n",
    "\n",
    "def get_list_of_indices(string):\n",
    "\tlis_words = string.split()\n",
    "\t# lis_ret = [ for word in lis_words]\n",
    "\tlis_ret = []\n",
    "\tfor word in lis_words:\n",
    "\t\ttry:\n",
    "\t\t\tind_append = dict_str2ind[word]\n",
    "\t\t\tlis_ret.append(ind_append)\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\t\t\t# ind_append = \n",
    "\t\t\t# print(\"THERE IT IS!\", word)\n",
    "\t# print(\"About to return\")\n",
    "\treturn lis_ret\n",
    "\n",
    "## **\n",
    "## **\n",
    "# read the word2vec representations\n",
    "\n",
    "with open('../review+wiki.filtered.200.txt') as f:\n",
    "\twordvecs = f.readlines()\n",
    "\n",
    "first_pair = wordvecs[0].split(\" \", 1)\n",
    "first_vec = convert_to_float(first_pair[1])\n",
    "dim_vecSpace = len(first_vec) # Dimension of the vector space in which we are\n",
    "\n",
    "# add stuff for EOS, Blank\n",
    "# at index = 0, 1\n",
    "\n",
    "eos_index = get_index()\n",
    "dict_str2ind[\"<EOS>\"] = eos_index\n",
    "dict_ind2str[eos_index] = \"<EOS>\"\n",
    "dict_ind2vec[eos_index] = [1.0]*dim_vecSpace\n",
    "\n",
    "blk_index = get_index()\n",
    "dict_str2ind[\"<BLANK>\"] = blk_index\n",
    "dict_ind2str[blk_index] = \"<BLANK>\"\n",
    "dict_ind2vec[blk_index] = [0.0]*dim_vecSpace\n",
    "\n",
    "\n",
    "for elem in wordvecs:\n",
    "\tliss = elem.split(\" \", 1) # split on the first space\n",
    "\tword_str = liss[0]\n",
    "\tword_vec = convert_to_float(liss[1])\n",
    "\t\n",
    "\there_index = get_index()\n",
    "\tdict_str2ind[word_str] = here_index\n",
    "\tdict_ind2str[here_index] = word_str\n",
    "\tdict_ind2vec[here_index] = word_vec\n",
    "\n",
    "# CHKING\n",
    "# print( dict_str2ind['a'] )\n",
    "\n",
    "## **\n",
    "## **\n",
    "# read the data\n",
    "\n",
    "with open('../reviews.aspect0.train.txt') as f:\n",
    "\ttrain_data = f.readlines()\n",
    "\n",
    "rating_regex = re.compile('\\d\\.\\d\\d \\d\\.\\d\\d \\d\\.\\d\\d \\d\\.\\d\\d \\d\\.\\d\\d\\t') # Exactly matches only the ratings\n",
    "\n",
    "# extract ratings - # each rating is a scalar value # NO ::: each rating is a list of 5 values\n",
    "ratings = [ float( re.findall(rating_regex, review)[0][0] ) for review in train_data ]\n",
    "\n",
    "# extract reviews\n",
    "reviews_str = [ rating_regex.sub('', review) for review in train_data ]\n",
    "reviews = [ get_list_of_indices( review_str ) for review_str in reviews_str ]\n",
    "X = reviews\n",
    "total_size = len(X)\n",
    "\n",
    "divide_train = int( (4*total_size)/5 )\n",
    "train_indices_of_X = sorted( random.sample( range(total_size), divide_train ) )\n",
    "\n",
    "X_train = []\n",
    "X_val = []\n",
    "ratings_train = []\n",
    "ratings_val = []\n",
    "for i in range(total_size):\n",
    "\tif i in train_indices_of_X:\n",
    "\t\tX_train.append(X[i])\n",
    "\t\tratings_train.append(ratings[i])\n",
    "\telse:\n",
    "\t\tX_val.append(X[i])\n",
    "\t\tratings_val.append(ratings[i])\n",
    "\n",
    "X = X_train\n",
    "ratings = ratings_train\n",
    "\n",
    "num_train_examples = len(X) # we also assume len(X) = len(ratings)\n",
    "num_val_examples = len(X_val)\n",
    "# read validation data\n",
    "\n",
    "# ** ** \n",
    "# ** ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAccuracy(X, ratings, attn_encoder):\n",
    "\n",
    "    # iterate through X_val and pass to generator->encoder to get mse_error and compare it to truth\n",
    "    num_val_examples = len(X)\n",
    "    X_val_size = num_val_examples\n",
    "    num_iters = X_val_size // (batch_size)\n",
    "    total_loss = 0.0\n",
    "    for iters in range(num_iters):\n",
    "\n",
    "        # get X_batch, ratings_batch\n",
    "        # This sampling also preserves the order\n",
    "        X_bch = []\n",
    "        ratings_bch = []\n",
    "\n",
    "        _ = [ ( X_bch.append(X[i]) , ratings_bch.append(ratings[i]) ) for i in sorted(random.sample(range(num_val_examples), batch_size)) ]\n",
    "\n",
    "        # almost done here - make all the reviews of equal length now\n",
    "\n",
    "        maxlen_rev = max(X_bch, key=len)\n",
    "        maxlen = len(maxlen_rev)\n",
    "\n",
    "        max_seq_len = min(maxlen, MASTER_MAX_VAL_LEN)\n",
    "        \n",
    "        \n",
    "        X_bach = np.empty([batch_size,max_seq_len])\n",
    "        ratings_bach = np.empty([batch_size,1])\n",
    "        \n",
    "        encoderLoss = nn.MSELoss(reduce=False)\n",
    "\n",
    "        for iterr in range(batch_size):\n",
    "            currentlen = len(X_bch[iterr])\n",
    "            if (currentlen < max_seq_len):\n",
    "                zero_count = max_seq_len - currentlen\n",
    "                X_bch[iterr].extend([0]*zero_count)\n",
    "            else:\n",
    "                X_bch[iterr] = X_bch[iterr][0:max_seq_len]\n",
    "            # X_bch[iterr] is now a list containing indices of words\n",
    "            # Convert it into a Variable ?\n",
    "            to_append = np.array( X_bch[iterr] )\n",
    "            X_bach[iterr] = to_append\n",
    "            to_append = np.array( ratings_bch[iterr] )\n",
    "            ratings_bach[iterr] = to_append\n",
    "        # X_bach is a 2d numpy array of size :: batch_size X maxlen\n",
    "\n",
    "        if (use_cuda):\n",
    "            X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor).cuda()\n",
    "            ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor).cuda()\n",
    "        else:\n",
    "            X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor)\n",
    "            ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor)\n",
    "\n",
    "        X_batch = Variable(X_bach_tensor)\n",
    "        ratings_batch = Variable(ratings_bach_tensor)\n",
    "\n",
    "#         init_hidden = generator.initHidden(batch_size, use_cuda)\n",
    "#         z_sample = generator.sample(X_batch, init_hidden, use_cuda)\n",
    "\n",
    "        attn_weights, ratings_pred = attn_encoder(X_batch, use_cuda, False)\n",
    "        encoder_loss = encoderLoss(ratings_pred, ratings_batch.squeeze(1))\n",
    "\n",
    "        total_loss += float(torch.sum(encoder_loss))\n",
    "\n",
    "    return total_loss / X_val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train function - here's some ingenuity\n",
    "# one iteration of training\n",
    "def train(X, ratings, attn_encoder, attn_encoder_optimizer, print_grad_norm):\n",
    "    # X - single batch\n",
    "\n",
    "    attn_encoder_optimizer.zero_grad()\n",
    "\n",
    "    encoderLoss = nn.MSELoss(reduce=False)\n",
    "\n",
    "    mean_cost = 0.0\n",
    "\n",
    "    attn_weights, ratings_pred = attn_encoder(X, use_cuda)\n",
    "    encoder_loss = encoderLoss(ratings_pred, ratings.squeeze(1))\n",
    "\n",
    "    cost = encoder_loss\n",
    "       \n",
    "    cost1 = torch.mean(cost)\n",
    "\n",
    "    mean_cost += float(cost1)\n",
    "\n",
    "    cost1.backward()\n",
    "        \n",
    "        \n",
    "    if (print_grad_norm):\n",
    "        for name, param in attn_encoder.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "                print(param.data.norm())\n",
    "                print (param.grad.data.norm())\n",
    "                \n",
    "        input()\n",
    "    \n",
    "    attn_encoder_optimizer.step()\n",
    "    \n",
    "    return mean_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(X, ratings, X_val, ratings_val, attn_encoder, learning_rate, learning_rate_decay, num_epochs, \\\n",
    "                load_dict=None, print_every=1000, plot_every=100, val_every=1000, print_grad_every=-1, \\\n",
    "                save_folder='', weight_decay=0):\n",
    "\n",
    "    \n",
    "    num_train_examples = len(X)\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0.0\n",
    "    plot_loss_total = 0.0\n",
    "    best_val_acc = float(\"inf\")\n",
    "    if load_dict is not None:\n",
    "        attn_encoder = load_dict['attn_encoder_model']\n",
    "        \n",
    "        cur_tot_iters = load_dict['tot_iter']\n",
    "    else:\n",
    "        cur_tot_iters = 0\n",
    "        \n",
    "    attn_enc_param_list = []\n",
    "    for param in attn_encoder.parameters():\n",
    "        if (param.requires_grad):\n",
    "            attn_enc_param_list.append(param)\n",
    "    attn_encoder_optimizer = optim.Adam(attn_enc_param_list, lr=learning_rate, weight_decay=weight_decay)    \n",
    "        \n",
    "    attn_encoder_scheduler = optim.lr_scheduler.StepLR(attn_encoder_optimizer, 1, learning_rate_decay)\n",
    "    \n",
    "    if (load_dict is not None):\n",
    "        attn_encoder_optimizer.load_state_dict(load_dict['attn_encoder_optimizer'])\n",
    "        for param_group in attn_encoder_optimizer.param_groups:\n",
    "            param_group['weight_decay'] = weight_decay\n",
    "            param_group['lr'] = learning_rate\n",
    "            \n",
    "        attn_encoder_scheduler.load_state_dict(load_dict['attn_encoder_scheduler'])\n",
    "\n",
    "#     print(encoder_optimizer.param_groups[0]['lr'])\n",
    "#      set iters_per_epoch\n",
    "    iters_per_epoch = num_train_examples // batch_size\n",
    "    n_iters = iters_per_epoch * num_epochs\n",
    "    \n",
    "    position_set = False\n",
    "    for epoch in range(num_epochs):\n",
    "        if (position_set):\n",
    "            pass\n",
    "#             encoder_scheduler.step()\n",
    "#             generator_scheduler.step()\n",
    "        for iter_num in range(iters_per_epoch):\n",
    "            if (cur_tot_iters >= epoch * iters_per_epoch + iter_num + 1):\n",
    "                continue\n",
    "            \n",
    "            position_set = True\n",
    "            # randomly choose sample from X and make them equal length\n",
    "            # This sampling also preserves the order\n",
    "            X_bch = []\n",
    "            ratings_bch = []\n",
    "\n",
    "            _ = [ ( X_bch.append(X[i]) , ratings_bch.append(ratings[i]) ) for i in sorted(random.sample(range(num_train_examples), batch_size)) ]\n",
    "\n",
    "            # almost done here - make all the reviews of equal length now\n",
    "\n",
    "            maxlen_rev = max(X_bch, key=len)\n",
    "            maxlen = len(maxlen_rev)\n",
    "\n",
    "            max_seq_len = min(maxlen, MASTER_MAX_LEN)\n",
    "            \n",
    "            X_bach = np.empty([batch_size,max_seq_len])\n",
    "            ratings_bach = np.empty([batch_size,1])\n",
    "\n",
    "            for iterr in range(batch_size):\n",
    "                currentlen = len(X_bch[iterr])\n",
    "                if (currentlen < max_seq_len):\n",
    "                    zero_count = max_seq_len - currentlen\n",
    "                    X_bch[iterr].extend([0]*zero_count)\n",
    "                else:\n",
    "                    X_bch[iterr] = X_bch[iterr][0:max_seq_len]\n",
    "                # X_bch[iterr] is now a list containing indices of words\n",
    "                # Convert it into a Variable ?\n",
    "                to_append = np.array( X_bch[iterr] )\n",
    "    # \t\t\t\tX_bach = np.append(X_bach, [to_append], axis = 0)\n",
    "                X_bach[iterr] = to_append\n",
    "                to_append = np.array( ratings_bch[iterr] )\n",
    "    # \t\t\t\tratings_bach = np.append(ratings_bach, to_append, axis = 0)\n",
    "                ratings_bach[iterr] = to_append\n",
    "            # X_bach is a 2d numpy array of size :: batch_size X maxlen\n",
    "\n",
    "            if (use_cuda):\n",
    "                X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor).cuda()\n",
    "                ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor).cuda()\n",
    "            else:\n",
    "                X_bach_tensor = torch.from_numpy(X_bach).type(torch.LongTensor)\n",
    "                ratings_bach_tensor = torch.from_numpy(ratings_bach).type(torch.FloatTensor)\n",
    "            X_batch = Variable(X_bach_tensor)\n",
    "            ratings_batch = Variable(ratings_bach_tensor)\n",
    "            # call train with this batch\n",
    "            cur_tot_iters = iter_num + 1 + epoch * iters_per_epoch\n",
    "            if (print_grad_every > 0 and cur_tot_iters % print_grad_every == 0):\n",
    "                cur_loss = train(X_batch, ratings_batch, attn_encoder, \\\n",
    "                                                   attn_encoder_optimizer, True)\n",
    "            else:\n",
    "                cur_loss = train(X_batch, ratings_batch, attn_encoder, \\\n",
    "                                                   attn_encoder_optimizer, False)\n",
    "            \n",
    "            print_loss_total += cur_loss\n",
    "            plot_loss_total += cur_loss\n",
    "\n",
    "            \n",
    "          \n",
    "            if (cur_tot_iters) % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, 1.0 * (cur_tot_iters) / n_iters),\n",
    "                                             cur_tot_iters, 1.0 * (cur_tot_iters) / n_iters * 100, print_loss_avg),flush=True)\n",
    "            \n",
    "            if (cur_tot_iters) % val_every == 0:\n",
    "                val_acc = getAccuracy(X_val, ratings_val, attn_encoder)\n",
    "                print(\"Val Acc: \", val_acc)\n",
    "                if (val_acc < best_val_acc):\n",
    "                    best_val_acc = val_acc\n",
    "                    best = True\n",
    "                else:\n",
    "                    best = False\n",
    "                    \n",
    "                save_dict = {}\n",
    "                save_dict['attn_encoder_model'] = attn_encoder\n",
    "                save_dict['attn_encoder_optimizer'] = attn_encoder_optimizer.state_dict()\n",
    "                save_dict['attn_encoder_scheduler'] = attn_encoder_scheduler.state_dict()\n",
    "                \n",
    "                save_dict['tot_iter'] = cur_tot_iters\n",
    "                save_dict['val_acc'] = val_acc\n",
    "                save_dict['best_so_far'] = best\n",
    "                torch.save(save_dict, save_folder+'chkpt_'+str(cur_tot_iters)+str(best))\n",
    "\n",
    "            if iter_num % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining pretrained_embeddings\n",
    "pretrained_embeddings = np.empty([len(dict_ind2vec), dim_vecSpace])\n",
    "for key in sorted(dict_ind2vec.keys()):\n",
    "    vec_here = dict_ind2vec[key]\n",
    "    pretrained_embeddings[key] = np.array(vec_here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu1604/temp/code/project_code/attn_model/AttnEncoder.py:49: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  hidden, (_, _) = self.lstm_i2h(x_transpose)\n"
     ]
    }
   ],
   "source": [
    "# Initialing hyperparam containers\n",
    "learning_rates = [0.00025883]\n",
    "length_regs = [0.0003]\n",
    "continuity_regs = [0.0006]\n",
    "learning_rate_decays = [1]\n",
    "weight_regs = [0]\n",
    "num_epochs = 60\n",
    "\n",
    "load_dict=None\n",
    "for lrate_decay in learning_rate_decays:\n",
    "    for length_reg in length_regs:\n",
    "        for continuity_reg in continuity_regs:\n",
    "            for l_rate in learning_rates:\n",
    "                for wt_reg in weight_regs:\n",
    "                    load_dict = torch.load('test1_' + str(wt_reg) + '/chkpt_60000False')\n",
    "                    attn_encoder = AttnEncoder.AttnEncoder(pretrained_embeddings, 200, 2, 50, 'LSTM', dropout=0.1)\n",
    "                    attn_encoder.float()\n",
    "\n",
    "                    if (use_cuda):\n",
    "                        attn_encoder.cuda()\n",
    "                    save_folder = 'test1_' + str(wt_reg) + '/'\n",
    "                    trainIters(X, ratings, X_val, ratings_val, attn_encoder, \n",
    "                                learning_rate=l_rate, learning_rate_decay=lrate_decay, num_epochs=num_epochs, \\\n",
    "                                print_every=100,val_every=1000,load_dict=load_dict, print_grad_every=-1, \\\n",
    "                              save_folder=save_folder,weight_decay=wt_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
